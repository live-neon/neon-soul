/**
 * Generic signal extraction with LLM-based semantic detection.
 * Uses LLM to identify identity signals (no keyword matching).
 * LLM provider required for all signal extraction operations.
 */

import type { Signal, SignalType } from '../types/signal.js';
import type { LLMProvider } from '../types/llm.js';
import { requireLLM } from '../types/llm.js';
import { embed } from './embeddings.js';
import { createSignalSource } from './provenance.js';
import type { MemoryFile } from './memory-walker.js';
import {
  classifyDimension as semanticClassifyDimension,
  classifySignalType as semanticClassifySignalType,
} from './semantic-classifier.js';

export interface ExtractionConfig {
  promptTemplate: string; // With {content}, {path}, {category} placeholders
  sourceType: 'template' | 'memory' | 'interview';
}

/**
 * Result from signal detection LLM call.
 */
interface SignalDetectionResult {
  isSignal: boolean;
  confidence: number;
}

interface ExtractedSignal {
  type: SignalType;
  text: string;
  confidence: number;
  line?: number;
}

/**
 * Extract signals from content using LLM analysis.
 * Note: In OpenClaw skill context, LLM calls go through OpenClaw's interface.
 */
export async function extractSignals(
  content: string,
  source: { file: string; line?: number; category?: string },
  config: ExtractionConfig
): Promise<Signal[]> {
  // Build prompt from template
  const prompt = config.promptTemplate
    .replace('{content}', content)
    .replace('{path}', source.file)
    .replace('{category}', source.category ?? 'general');

  // In actual skill execution, this would call OpenClaw's LLM interface.
  // For now, we provide a stub that returns empty (to be integrated).
  const extracted = await callLLMForSignals(prompt);

  // Generate embeddings and build Signal objects
  const signals: Signal[] = [];

  for (const ext of extracted) {
    const embedding = await embed(ext.text);
    const signalSource = createSignalSource(
      source.file,
      ext.line ?? source.line ?? 0,
      content.slice(0, 100) // First 100 chars as context
    );

    signals.push({
      id: generateId(),
      type: ext.type,
      text: ext.text,
      confidence: ext.confidence,
      embedding,
      source: signalSource,
    });
  }

  return signals;
}

/**
 * Placeholder for OpenClaw LLM integration.
 * In skill execution context, this calls OpenClaw's authenticated LLM.
 */
async function callLLMForSignals(
  _prompt: string
): Promise<ExtractedSignal[]> {
  // TODO: Integrate with OpenClaw skill LLM interface
  // This is called during skill execution where OpenClaw provides LLM access
  return [];
}

// MN-2 FIX: Use crypto.randomUUID() for better collision resistance
import { randomUUID } from 'node:crypto';

/**
 * Generate unique ID for signals.
 * Uses crypto.randomUUID() for proper collision resistance.
 */
function generateId(): string {
  return `sig_${randomUUID()}`;
}

// TR-4: Using shared requireLLM from llm.ts (removed local duplicate)

/**
 * IM-7 FIX: Sanitize user input to prevent prompt injection.
 * Escapes XML-like tags in user content.
 */
function sanitizeForPrompt(text: string): string {
  return text.replace(/</g, '&lt;').replace(/>/g, '&gt;');
}

/**
 * Detect if a line is an identity signal using LLM.
 * Returns detection result with confidence score.
 * IM-7 FIX: Uses XML delimiters to prevent prompt injection.
 */
async function isIdentitySignal(
  llm: LLMProvider,
  line: string
): Promise<SignalDetectionResult> {
  // IM-7 FIX: Sanitize and wrap user content in XML delimiters
  const sanitizedLine = sanitizeForPrompt(line);
  const prompt = `Is this line an identity signal? An identity signal is a statement that reveals:
- Core values, beliefs, or principles
- Preferences or inclinations
- Goals or aspirations
- Boundaries or constraints
- Relationship patterns
- Behavioral patterns or habits

<user_content>
${sanitizedLine}
</user_content>

Answer yes or no based on the content in <user_content>, with a confidence from 0.0 to 1.0.`;

  const result = await llm.classify(prompt, {
    categories: ['yes', 'no'] as const,
    context: 'Identity signal detection',
  });

  return {
    isSignal: result.category === 'yes',
    confidence: result.confidence,
  };
}

/** Default confidence threshold for signal detection */
const DEFAULT_CONFIDENCE_THRESHOLD = 0.5;

/** Batch size for parallel processing */
const BATCH_SIZE = 10;

/**
 * Extract signals from markdown content using LLM-based semantic detection.
 * LLM provider is required - no fallback to keyword matching.
 *
 * Performance optimizations (CR6-1):
 * - Collects candidate lines first, then batch processes
 * - Parallelizes dimension + signalType classification (independent operations)
 * - Processes detection in parallel batches
 *
 * @param llm - LLM provider (required)
 * @param content - Markdown content to extract signals from
 * @param source - Source file information
 * @param options - Optional configuration
 * @returns Array of extracted signals
 * @throws LLMRequiredError if llm is null/undefined
 */
export async function extractSignalsFromContent(
  llm: LLMProvider | null | undefined,
  content: string,
  source: { file: string; category?: string },
  options: { confidenceThreshold?: number } = {}
): Promise<Signal[]> {
  requireLLM(llm, 'extractSignalsFromContent');

  const confidenceThreshold =
    options.confidenceThreshold ?? DEFAULT_CONFIDENCE_THRESHOLD;

  // Phase 1: Collect candidate lines (no LLM calls yet)
  const candidates: Array<{ text: string; lineNum: number; originalLine: string }> =
    [];
  const lines = content.split('\n');

  for (let i = 0; i < lines.length; i++) {
    const line = lines[i]?.trim() ?? '';
    if (!line || line.length < 10) continue;

    // Extract text from structured markdown
    let text = line;

    // Strip bullet point markers
    if (line.startsWith('- ') || line.startsWith('* ') || /^\d+\.\s/.test(line)) {
      text = line.replace(/^[-*]\s+|\d+\.\s+/, '');
    }

    // Strip heading markers
    if (line.startsWith('#')) {
      text = line.replace(/^#+\s*/, '');
    }

    // Skip short text after stripping
    if (text.length < 10) continue;

    candidates.push({ text, lineNum: i + 1, originalLine: line });
  }

  // Phase 2: Detect identity signals in parallel batches
  const detectionResults: Array<{
    candidate: (typeof candidates)[0];
    detection: { isSignal: boolean; confidence: number };
  }> = [];

  for (let i = 0; i < candidates.length; i += BATCH_SIZE) {
    const batch = candidates.slice(i, i + BATCH_SIZE);
    const batchResults = await Promise.all(
      batch.map(async (candidate) => ({
        candidate,
        detection: await isIdentitySignal(llm, candidate.text),
      }))
    );
    detectionResults.push(...batchResults);
  }

  // Phase 3: Filter to confirmed signals
  const confirmedSignals = detectionResults.filter(
    (r) => r.detection.isSignal && r.detection.confidence >= confidenceThreshold
  );

  // Phase 4: Classify and embed confirmed signals in parallel
  const signals: Signal[] = await Promise.all(
    confirmedSignals.map(async ({ candidate, detection }) => {
      // Parallelize dimension, signalType, and embedding (independent operations)
      const [dimension, signalType, embedding] = await Promise.all([
        semanticClassifyDimension(llm, candidate.text),
        semanticClassifySignalType(llm, candidate.text),
        embed(candidate.text),
      ]);

      const signalSource = createSignalSource(
        source.file,
        candidate.lineNum,
        candidate.originalLine.slice(0, 100)
      );

      return {
        id: generateId(),
        type: signalType,
        text: candidate.text,
        confidence: detection.confidence,
        embedding,
        source: signalSource,
        dimension,
      };
    })
  );

  return signals;
}

/**
 * Extract signals from multiple memory files.
 * LLM provider is required for all extraction operations.
 *
 * @param llm - LLM provider (required)
 * @param memoryFiles - Array of memory files to process
 * @returns Array of extracted signals from all files
 * @throws LLMRequiredError if llm is null/undefined
 */
export async function extractSignalsFromMemoryFiles(
  llm: LLMProvider | null | undefined,
  memoryFiles: MemoryFile[]
): Promise<Signal[]> {
  requireLLM(llm, 'extractSignalsFromMemoryFiles');

  // TR-2: Parallelize file-level extraction (files are independent)
  const signalArrays = await Promise.all(
    memoryFiles.map((file) =>
      extractSignalsFromContent(llm, file.content, {
        file: file.path,
        category: file.category,
      })
    )
  );

  return signalArrays.flat();
}
